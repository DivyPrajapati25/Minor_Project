{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa129kTvAjaM"
      },
      "outputs": [],
      "source": [
        "# # Fraud Detection using ML, DL, GNN with Class Balancing & Anomaly Detection\n",
        "\n",
        "# # Required Libraries\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "# !pip install torch-geometric\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import os\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import networkx as nx\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, r2_score\n",
        "# from sklearn.ensemble import IsolationForest\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# from torch_geometric.data import Data\n",
        "# from torch_geometric.utils import from_networkx\n",
        "# from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
        "\n",
        "# # Load Data\n",
        "# data = pd.read_csv(\"/content/PS_20174392719_1491204439457_log.csv\")\n",
        "# data = data[data['isFraud'].notna()]\n",
        "# data['isFraud'] = data['isFraud'].astype(int)\n",
        "# data = data[data['type'].isin(['TRANSFER', 'CASH_OUT'])]\n",
        "# data.drop(['step', 'type', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1, inplace=True)\n",
        "# data.dropna(inplace=True)\n",
        "\n",
        "# # Feature Scaling\n",
        "# scaler = StandardScaler()\n",
        "# features = ['amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest']\n",
        "# data[features] = scaler.fit_transform(data[features])\n",
        "\n",
        "# # Define X and y\n",
        "# X = data.drop('isFraud', axis=1)\n",
        "# y = data['isFraud']\n",
        "\n",
        "# # Apply SMOTE\n",
        "# sm = SMOTE(random_state=42)\n",
        "# X_res, y_res = sm.fit_resample(X, y)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, stratify=y_res)\n",
        "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float)\n",
        "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "# X_test_tensor = torch.tensor(X_test.values, dtype=torch.float)\n",
        "# y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# # DNN Models\n",
        "# class FNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(FNN, self).__init__()\n",
        "#         self.net = nn.Sequential(\n",
        "#             nn.Linear(5, 64), nn.ReLU(),\n",
        "#             nn.Linear(64, 32), nn.ReLU(),\n",
        "#             nn.Linear(32, 2))\n",
        "#     def forward(self, x):\n",
        "#         return self.net(x)\n",
        "\n",
        "# class CNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(CNN, self).__init__()\n",
        "#         self.conv = nn.Sequential(\n",
        "#             nn.Conv1d(1, 32, kernel_size=2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Flatten())\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(32 * 4, 64), nn.ReLU(),\n",
        "#             nn.Linear(64, 2))\n",
        "#     def forward(self, x):\n",
        "#         x = x.unsqueeze(1)\n",
        "#         x = self.conv(x)\n",
        "#         return self.fc(x)\n",
        "\n",
        "# class RNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(RNN, self).__init__()\n",
        "#         self.rnn = nn.RNN(input_size=5, hidden_size=64, batch_first=True)\n",
        "#         self.fc = nn.Linear(64, 2)\n",
        "#     def forward(self, x):\n",
        "#         x = x.unsqueeze(1)\n",
        "#         out, _ = self.rnn(x)\n",
        "#         return self.fc(out[:, -1, :])\n",
        "\n",
        "# def train_dnn(model, X_train, y_train, X_test, y_test):\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#     loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 10.0]))\n",
        "#     for _ in range(5):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(X_train)\n",
        "#         loss = loss_fn(output, y_train)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         probs = torch.softmax(model(X_test), dim=1)[:, 1]\n",
        "#         preds = (probs > 0.4).int()\n",
        "#         return preds\n",
        "\n",
        "# for name, model in {\"FNN\": FNN(), \"CNN\": CNN(), \"RNN\": RNN()}.items():\n",
        "#     print(f\"\\n{name}:\")\n",
        "#     preds = train_dnn(model, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
        "#     print(\"Accuracy:\", accuracy_score(y_test_tensor, preds))\n",
        "#     print(\"F1:\", f1_score(y_test_tensor, preds))\n",
        "#     print(\"Precision:\", precision_score(y_test_tensor, preds))\n",
        "#     print(\"Recall:\", recall_score(y_test_tensor, preds))\n",
        "#     print(\"R2:\", r2_score(y_test_tensor, preds))\n",
        "\n",
        "# # GNN Models\n",
        "# class GCNModel(nn.Module):\n",
        "#     def __init__(self, in_channels):\n",
        "#         super(GCNModel, self).__init__()\n",
        "#         self.conv1 = GCNConv(in_channels, 64)\n",
        "#         self.conv2 = GCNConv(64, 32)\n",
        "#         self.lin = nn.Linear(32, 2)\n",
        "#     def forward(self, x, edge_index, edge_attr=None):\n",
        "#         x = torch.relu(self.conv1(x, edge_index))\n",
        "#         x = torch.relu(self.conv2(x, edge_index))\n",
        "#         return self.lin(x)\n",
        "\n",
        "# class GATModel(nn.Module):\n",
        "#     def __init__(self, in_channels):\n",
        "#         super(GATModel, self).__init__()\n",
        "#         self.gat1 = GATConv(in_channels, 64, heads=2)\n",
        "#         self.gat2 = GATConv(128, 32, heads=1)\n",
        "#         self.lin = nn.Linear(32, 2)\n",
        "#     def forward(self, x, edge_index, edge_attr=None):\n",
        "#         x = torch.relu(self.gat1(x, edge_index))\n",
        "#         x = torch.relu(self.gat2(x, edge_index))\n",
        "#         return self.lin(x)\n",
        "\n",
        "# class SAGEModel(nn.Module):\n",
        "#     def __init__(self, in_channels):\n",
        "#         super(SAGEModel, self).__init__()\n",
        "#         self.sage1 = SAGEConv(in_channels, 64)\n",
        "#         self.sage2 = SAGEConv(64, 32)\n",
        "#         self.lin = nn.Linear(32, 2)\n",
        "#     def forward(self, x, edge_index, edge_attr=None):\n",
        "#         x = torch.relu(self.sage1(x, edge_index))\n",
        "#         x = torch.relu(self.sage2(x, edge_index))\n",
        "#         return self.lin(x)\n",
        "\n",
        "# class LSTMGCNHybrid(nn.Module):\n",
        "#     def __init__(self, in_channels):\n",
        "#         super().__init__()\n",
        "#         self.lstm = nn.LSTM(in_channels, 64, batch_first=True)\n",
        "#         self.gcn = GCNConv(64, 32)\n",
        "#         self.lin = nn.Linear(32, 2)\n",
        "#     def forward(self, x, edge_index, edge_attr=None):\n",
        "#         x = x.unsqueeze(1)\n",
        "#         x, _ = self.lstm(x)\n",
        "#         x = x.squeeze(1)\n",
        "#         x = torch.relu(self.gcn(x, edge_index))\n",
        "#         return self.lin(x)\n",
        "\n",
        "# # Dummy graph for demonstration\n",
        "# graph = nx.DiGraph()\n",
        "# for i in range(len(X)):\n",
        "#     graph.add_edge(f\"n{i}\", f\"m{i}\", amount=X.iloc[i][0], isFraud=y.iloc[i])\n",
        "#     graph.nodes[f\"n{i}\"]['x'] = [0.0]\n",
        "#     graph.nodes[f\"m{i}\"]['x'] = [0.0]\n",
        "# data_g = from_networkx(graph)\n",
        "# data_g.edge_attr = torch.tensor([[e['amount']] for _, _, e in graph.edges(data=True)], dtype=torch.float)\n",
        "# data_g.y = torch.tensor([e['isFraud'] for _, _, e in graph.edges(data=True)], dtype=torch.long)\n",
        "# data_g.x = torch.tensor([[0.0]] * data_g.num_nodes, dtype=torch.float)\n",
        "\n",
        "# def train_gnn(model, data):\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#     loss_fn = nn.CrossEntropyLoss()\n",
        "#     for _ in range(5):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         out = model(data.x, data.edge_index, data.edge_attr)\n",
        "#         pred = out[data.edge_index[1]]\n",
        "#         loss = loss_fn(pred, data.y)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         pred = model(data.x, data.edge_index, data.edge_attr)[data.edge_index[1]].argmax(dim=1)\n",
        "#         print(\"Accuracy:\", accuracy_score(data.y, pred))\n",
        "#         print(\"F1:\", f1_score(data.y, pred))\n",
        "#         print(\"Precision:\", precision_score(data.y, pred))\n",
        "#         print(\"Recall:\", recall_score(data.y, pred))\n",
        "#         print(\"R2:\", r2_score(data.y, pred))\n",
        "\n",
        "# for name, model in {\"GCN\": GCNModel(1), \"GAT\": GATModel(1), \"GraphSAGE\": SAGEModel(1), \"LSTM+GCN\": LSTMGCNHybrid(1)}.items():\n",
        "#     print(f\"\\n{name}:\")\n",
        "#     train_gnn(model, data_g)\n",
        "\n",
        "# # Anomaly Detection\n",
        "# iso = IsolationForest(contamination=0.001)\n",
        "# y_pred_iso = iso.fit_predict(X)\n",
        "# y_pred_iso = np.where(y_pred_iso == -1, 1, 0)\n",
        "# print(\"\\nIsolation Forest:\")\n",
        "# print(\"Recall:\", recall_score(y, y_pred_iso))\n",
        "\n",
        "# # Final Conclusion\n",
        "# print(\"\\nConclusion:\")\n",
        "# print(\"1. SMOTE improves fraud detection by balancing class distribution.\")\n",
        "# print(\"2. DNNs benefit from threshold tuning and weighted loss.\")\n",
        "# print(\"3. GNNs need more semantic graph structure to perform well.\")\n",
        "# print(\"4. Anomaly detection gives useful outlier insight, but low precision.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZRCi777qlF0",
        "outputId": "b90daba7-e1b8-48e9-f59b-097d59d77506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (494 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.0/494.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt20cpu\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.18%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt20cpu\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, r2_score\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv"
      ],
      "metadata": {
        "id": "d7Po-Jg7qmYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88028e01-cc94-4027-fa6c-a9555b077c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "\n",
        "\n",
        "# Clean column names\n",
        " # Debug: show actual column names\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load your data\n",
        "data = pd.read_csv(\"/content/Synthetic_Financial_datasets_log.csv\")\n",
        "data.columns = data.columns.str.strip()\n",
        "print(\"Columns:\", data.columns.tolist())  # See actual columns\n",
        "\n",
        "# Ensure the target column exists and is numeric\n",
        "if 'Fraud_Label' in data.columns:\n",
        "    data = data[data['Fraud_Label'].notna()]\n",
        "    data['Fraud_Label'] = data['Fraud_Label'].astype(int)\n",
        "\n",
        "# Drop irrelevant or non-existent columns safely\n",
        "for col in ['step', 'type', 'nameOrig', 'nameDest', 'isFlaggedFraud']:\n",
        "    if col in data.columns:\n",
        "        data.drop(col, axis=1, inplace=True)\n",
        "\n",
        "# Drop any remaining missing values\n",
        "data.dropna(inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "nyqofBfXqqWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a50370-6b70-4bb0-cc2e-0be1878c034d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show exact columns\n",
        "print(\"Actual columns in dataset:\", data.columns.tolist())\n",
        "\n",
        "# Try fuzzy matching to find similar column names\n",
        "from difflib import get_close_matches\n",
        "\n",
        "desired_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
        "for feat in desired_features:\n",
        "    print(f\"Closest match for '{feat}':\", get_close_matches(feat, data.columns.tolist(), n=1))\n"
      ],
      "metadata": {
        "id": "mwkGwTkDqsTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e289c95a-989a-4373-b363-7a94621df9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual columns in dataset: ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFraud']\n",
            "Closest match for 'amount': ['amount']\n",
            "Closest match for 'oldbalanceOrg': ['oldbalanceOrg']\n",
            "Closest match for 'newbalanceOrig': ['newbalanceOrig']\n",
            "Closest match for 'oldbalanceDest': ['oldbalanceDest']\n",
            "Closest match for 'newbalanceDest': ['newbalanceDest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"/content/Synthetic_Financial_datasets_log.csv\")\n",
        "data.columns = data.columns.str.strip()  # Clean column names\n",
        "\n",
        "# Identify the label column\n",
        "label_col = None\n",
        "for possible_label in ['isFraud', 'Fraud_Label']:\n",
        "    if possible_label in data.columns:\n",
        "        label_col = possible_label\n",
        "        break\n",
        "\n",
        "if label_col is None:\n",
        "    raise ValueError(\"Label column 'isFraud' or 'Fraud_Label' not found.\")\n",
        "\n",
        "# Filter by type if exists\n",
        "if 'type' in data.columns:\n",
        "    data = data[data['type'].isin(['TRANSFER', 'CASH_OUT'])]\n",
        "    data.drop(['type'], axis=1, inplace=True)\n",
        "\n",
        "# Drop optional columns if they exist\n",
        "drop_cols = ['step', 'nameOrig', 'nameDest', 'isFlaggedFraud']\n",
        "data.drop([col for col in drop_cols if col in data.columns], axis=1, inplace=True)\n",
        "\n",
        "# Clean missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Convert label to int\n",
        "data[label_col] = data[label_col].astype(int)\n",
        "\n",
        "# Normalize numeric features (only if they exist)\n",
        "desired_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
        "available_features = [feat for feat in desired_features if feat in data.columns]\n",
        "\n",
        "print(\"Scaling these features:\", available_features)\n",
        "\n",
        "if available_features:\n",
        "    scaler = StandardScaler()\n",
        "    data[available_features] = scaler.fit_transform(data[available_features])\n",
        "else:\n",
        "    print(\"⚠️ No features available for scaling.\")\n",
        "\n",
        "# Final dataset split\n",
        "X = data.drop(label_col, axis=1)\n",
        "y = data[label_col]\n",
        "\n",
        "print(\"✅ Shape of X:\", X.shape)\n",
        "print(\"✅ Class distribution in y:\\n\", y.value_counts())\n"
      ],
      "metadata": {
        "id": "pAzmMHJyquqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb5676e-cb5a-4274-b2db-855f84a206df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-1445930222>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data.drop(['type'], axis=1, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaling these features: ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
            "✅ Shape of X: (838184, 5)\n",
            "✅ Class distribution in y:\n",
            " isFraud\n",
            "0    836294\n",
            "1      1890\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only numeric columns in X before SMOTE\n",
        "X = X.select_dtypes(include=['number'])\n",
        "\n",
        "# Now safely apply SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sm = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, stratify=y_res, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"✅ Resampled class distribution:\\n\", y_res.value_counts())\n",
        "print(\"✅ Training set size:\", X_train.shape)\n"
      ],
      "metadata": {
        "id": "jV7iGFFHqut_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56554117-9aa1-4dc9-fe46-aef5e5a0b228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Resampled class distribution:\n",
            " isFraud\n",
            "1    836294\n",
            "0    836294\n",
            "Name: count, dtype: int64\n",
            "✅ Training set size: (1338070, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "DumVJS19quzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FNNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.fc1(x))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(64 * (sequence_length // 2), num_classes)  # adjust sequence_length\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  # last time-step\n",
        "        return out\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for data, labels in dataloader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "jjODj0niqu1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dnn(model, X_train, y_train, X_test, y_test):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 10.0]))\n",
        "    for _ in range(5):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_train)\n",
        "        loss = loss_fn(output, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = torch.softmax(model(X_test), dim=1)[:, 1]\n",
        "        preds = (probs > 0.4).int()\n",
        "        return preds"
      ],
      "metadata": {
        "id": "JprS47lMqu4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FNN(nn.Module):\n",
        "    def __init__(self, input_dim=11):  # match with X_train_tensor.shape[1]\n",
        "        super(FNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "FYnoL5cHqu6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import networkx as nx\n",
        "import torch\n",
        "from torch_geometric.utils import from_networkx\n",
        "\n",
        "# Step 1: Load Data\n",
        "file_path = '/content/Synthetic_Financial_datasets_log.csv'  # change path if needed\n",
        "data = pd.read_csv(file_path)\n",
        "data.columns = data.columns.str.strip()  # Clean column names\n",
        "print(\"🔍 Columns in dataset:\", data.columns.tolist())\n",
        "\n",
        "# Step 2: Identify Required Columns\n",
        "required_cols = ['Transaction_Amount', 'Account_Balance', 'Risk_Score', 'Transaction_Distance', 'Fraud_Label']\n",
        "available_cols = [col for col in required_cols if col in data.columns]\n",
        "missing_cols = [col for col in required_cols if col not in data.columns]\n",
        "\n",
        "if missing_cols:\n",
        "    print(f\"⚠️ Warning: Missing required columns: {missing_cols}\")\n",
        "if len(available_cols) < 2 or 'Fraud_Label' not in available_cols:\n",
        "    raise ValueError(\"🚫 Not enough valid columns to continue. Please check your dataset.\")\n",
        "\n",
        "# Step 3: Drop Irrelevant Columns\n",
        "drop_cols = ['Transaction_ID', 'User_ID', 'Timestamp', 'Location', 'Card_Type', 'Authentication_Method']\n",
        "data.drop(columns=[col for col in drop_cols if col in data.columns], inplace=True)\n",
        "\n",
        "# Step 4: Scale Numerical Features\n",
        "features = ['Transaction_Amount', 'Account_Balance', 'Risk_Score', 'Transaction_Distance']\n",
        "scaler = StandardScaler()\n",
        "data[features] = scaler.fit_transform(data[features])\n",
        "\n",
        "# Step 5: Split into X and y\n",
        "label_col = 'Fraud_Label'\n",
        "X = data[features].copy()\n",
        "y = data[label_col].astype(int)\n",
        "\n",
        "# Step 6: Apply SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(X, y)\n",
        "\n",
        "# Step 7: Create Graph\n",
        "graph = nx.DiGraph()\n",
        "for i in range(len(X_res)):\n",
        "    amt = X_res.iloc[i].get('Transaction_Amount', 0.0)\n",
        "    fraud = y_res.iloc[i]\n",
        "    graph.add_edge(f\"n{i}\", f\"m{i}\", amount=amt, isFraud=fraud)\n",
        "    graph.nodes[f\"n{i}\"]['x'] = [0.0]\n",
        "    graph.nodes[f\"m{i}\"]['x'] = [0.0]\n",
        "\n",
        "# Step 8: Convert to PyTorch Geometric Data\n",
        "data_g = from_networkx(graph)\n",
        "data_g.edge_attr = torch.tensor([[e['amount']] for _, _, e in graph.edges(data=True)], dtype=torch.float)\n",
        "data_g.y = torch.tensor([e['isFraud'] for _, _, e in graph.edges(data=True)], dtype=torch.long)\n",
        "data_g.x = torch.tensor([[0.0]] * data_g.num_nodes, dtype=torch.float)\n",
        "\n",
        "# ✅ Final Printout\n",
        "print(data_g)\n",
        "print(f\"📊 Number of nodes: {data_g.num_nodes}\")\n",
        "print(f\"🔗 Number of edges: {data_g.num_edges}\")\n",
        "print(f\"📈 Fraud class distribution: {torch.bincount(data_g.y)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EirtPWckqu8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "8d00c113-505e-4eea-daf8-b1a2877feafa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Columns in dataset: ['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n",
            "⚠️ Warning: Missing required columns: ['Transaction_Amount', 'Account_Balance', 'Risk_Score', 'Transaction_Distance', 'Fraud_Label']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "🚫 Not enough valid columns to continue. Please check your dataset.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3562713831>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"⚠️ Warning: Missing required columns: {missing_cols}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'Fraud_Label'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚫 Not enough valid columns to continue. Please check your dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Step 3: Drop Irrelevant Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 🚫 Not enough valid columns to continue. Please check your dataset."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gnn(model, data):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for _ in range(5):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_attr)\n",
        "        pred = out[data.edge_index[1]]\n",
        "        loss = loss_fn(pred, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = model(data.x, data.edge_index, data.edge_attr)[data.edge_index[1]].argmax(dim=1)\n",
        "        print(\"Accuracy:\", accuracy_score(data.y, pred))\n",
        "        print(\"F1:\", f1_score(data.y, pred))\n",
        "        print(\"Precision:\", precision_score(data.y, pred))\n",
        "        print(\"Recall:\", recall_score(data.y, pred))\n",
        "        print(\"R2:\", r2_score(data.y, pred))"
      ],
      "metadata": {
        "id": "9t94aHUQqu-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "# Load dataset\n",
        "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
        "data_g = dataset[0]  # Cora dataset has only one graph\n",
        "input_dim = dataset.num_node_features\n",
        "num_classes = dataset.num_classes  # Cora has 7 classes\n",
        "\n",
        "# GCN Model\n",
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, 16)\n",
        "        self.conv2 = GCNConv(16, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# GAT Model\n",
        "class GATModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(GATModel, self).__init__()\n",
        "        self.gat1 = GATConv(input_dim, 8, heads=4, concat=True)\n",
        "        self.gat2 = GATConv(32, num_classes, heads=1, concat=False)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.elu(self.gat1(x, edge_index))\n",
        "        x = self.gat2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# GraphSAGE Model\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class ImprovedGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(ImprovedGraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# LSTM + GCN Hybrid Model\n",
        "class LSTMGCN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, gcn_in, gcn_out, num_classes):\n",
        "        super(LSTMGCN, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
        "        self.gcn = GCNConv(gcn_in, gcn_out)\n",
        "        self.fc = nn.Linear(hidden_size + gcn_out, num_classes)\n",
        "\n",
        "    def forward(self, x_seq, x_graph, edge_index):\n",
        "        lstm_out, _ = self.lstm(x_seq)\n",
        "        lstm_feat = lstm_out[:, -1, :]  # last timestep\n",
        "\n",
        "        gcn_feat = self.gcn(x_graph, edge_index)\n",
        "\n",
        "        combined = torch.cat([lstm_feat, gcn_feat], dim=1)\n",
        "        out = self.fc(combined)\n",
        "        return out\n",
        "\n",
        "# Training Function\n",
        "def train_gnn(model, data):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(20):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Main Loop for Training and Evaluation\n",
        "\n",
        "  # Main Loop for Training and Evaluation\n",
        "\n",
        "models = {\n",
        "    \"GCNModel\": GCNModel(input_dim),\n",
        "    \"GAT\": GATModel(input_dim),\n",
        "    \"GraphSAGE\": ImprovedGraphSAGE(input_dim, 16, num_classes),\n",
        "    \"LSTM+GCN\": LSTMGCN(input_size=input_dim, hidden_size=16, gcn_in=input_dim, gcn_out=16, num_classes=num_classes)\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    train_gnn(model, data_g)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data_g)\n",
        "        y_pred = out.argmax(dim=1)\n",
        "        y_true = data_g.y\n",
        "\n",
        "        y_pred_test = y_pred[data_g.test_mask].cpu()\n",
        "        y_true_test = y_true[data_g.test_mask].cpu()\n",
        "\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(confusion_matrix(y_true_test, y_pred_test))\n",
        "\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(y_true_test, y_pred_test, digits=4))\n",
        "\n",
        "        try:\n",
        "            print(\"AUC-ROC Score:\")\n",
        "            print(roc_auc_score(y_true_test, y_pred_test, multi_class='ovr'))\n",
        "        except ValueError:\n",
        "            print(\"⚠️ AUC-ROC not defined (only one class in predictions).\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a5WeGZINqvBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(data_g)\n",
        "    y_scores = F.softmax(out, dim=1)  # Get probability distribution\n",
        "    y_pred = y_scores.argmax(dim=1)\n",
        "    y_true = data_g.y\n",
        "\n",
        "    y_pred_test = y_pred[data_g.test_mask].cpu()\n",
        "    y_true_test = y_true[data_g.test_mask].cpu()\n",
        "    y_scores_test = y_scores[data_g.test_mask].cpu()\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_true_test, y_pred_test))\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true_test, y_pred_test, digits=4, zero_division=0))\n",
        "\n",
        "    try:\n",
        "        # Binarize labels for ROC AUC\n",
        "        y_true_bin = label_binarize(y_true_test, classes=list(range(num_classes)))\n",
        "        print(\"AUC-ROC Score:\")\n",
        "        print(roc_auc_score(y_true_bin, y_scores_test, multi_class='ovr'))\n",
        "    except ValueError as e:\n",
        "        print(f\"⚠️ AUC-ROC not defined: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "skpU-wAX683z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iso = IsolationForest(contamination=0.001)\n",
        "y_pred_iso = iso.fit_predict(X)\n",
        "y_pred_iso = np.where(y_pred_iso == -1, 1, 0)\n",
        "print(\"\\nIsolation Forest:\")\n",
        "print(\"Recall:\", recall_score(y, y_pred_iso))"
      ],
      "metadata": {
        "id": "2eBixw1lqvEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "def evaluate_model(preds, labels, probs=None):\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(labels, preds))\n",
        "    print(\"Classification Report:\\n\", classification_report(labels, preds, digits=4))\n",
        "    if probs is not None:\n",
        "        try:\n",
        "            print(\"AUC-ROC Score:\", roc_auc_score(labels, probs))\n",
        "        except:\n",
        "            print(\"⚠️ AUC undefined.\")\n"
      ],
      "metadata": {
        "id": "UuyfZH_nf0xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, row in X_res.iterrows():\n",
        "    user = row['User_ID'] if 'User_ID' in row else f\"user_{i % 10}\"\n",
        "    merchant = row['Merchant_ID'] if 'Merchant_ID' in row else f\"merchant_{i % 5}\"\n",
        "    graph.add_edge(user, merchant, amount=row['Transaction_Amount'], isFraud=int(y_res.iloc[i]))\n"
      ],
      "metadata": {
        "id": "aj1ES1hJfsoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "def record_results(name, y_true, y_pred):\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"F1\": f1_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred),\n",
        "        \"Recall\": recall_score(y_true, y_pred)\n",
        "    })\n",
        "\n",
        "# After evaluation:\n",
        "pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "IONt_vBzfdJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nConclusion:\")\n",
        "print(\"1. SMOTE improves fraud detection by balancing class distribution.\")\n",
        "print(\"2. DNNs benefit from threshold tuning and weighted loss.\")\n",
        "print(\"3. GNNs need more semantic graph structure to perform well.\")\n",
        "print(\"4. Anomaly detection gives useful outlier insight, but low precision.\")"
      ],
      "metadata": {
        "id": "lUtFFr1cqvPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}